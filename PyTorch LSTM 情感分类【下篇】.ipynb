{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现流程：\n",
    "\n",
    "#### 1. 读取原始数据集（文本集）\n",
    "\n",
    "#### 2. 文本预处理\n",
    "* **2.1 清理无用的标点符号**\n",
    "* **2.2 根据 换行符 \\n 分割**\n",
    "* **2.3 单词 --> 索引 转换**\n",
    "* **2.4 标签 --> 1， 0 转换**\n",
    "* **2.5 清理文本太短以及过长的样本**\n",
    "* **2.6 将单词映射为整型**\n",
    "* **2.7 设定统一的文本长度，对整个文本数据中的每条评论进行填充或截断**\n",
    "\n",
    "#### 3. 特征工程\n",
    "* **3.1 array --> tensor**\n",
    "* **3.2 将数据集分离成：train, val, test 三部分，比例是： 0.8, 0.1, 0.1**\n",
    "* **3.3 通过DataLoader按批处理数据**\n",
    "\n",
    "#### 4. 定义网络模型结构\n",
    "\n",
    "#### 5. 定义超参数\n",
    "\n",
    "#### 6. 定义训练函数（训练 + 验证）\n",
    "\n",
    "#### 7. 定义测试函数\n",
    "\n",
    "#### 8. 定义预测函数\n",
    "\n",
    "----------------------------\n",
    "\n",
    "* **[B站账号： 唐国梁Tommy]** <https://space.bilibili.com/474347248/channel/index>\n",
    "* **代码+数据集下载，请查看我的B站个人简介**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载文本和标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本数据\n",
    "with open(\"data/reviews.txt\", 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33678267"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) # 共33678267个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text) # 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell h'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10] # 显示前10个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取标签数据\n",
    "with open('data/labels.txt', 'r') as file:\n",
    "    labels = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels) # 共225000个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels) # 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive\\nn'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10] # 显示前10个字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 数据 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标点符号 :  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# 2.1 清理无用的标点符号\n",
    "from string import punctuation\n",
    "\n",
    "print(\"标点符号 : \", punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = ''.join([char for char in text if char not in punctuation]) # 遍历文本中每一个字符，跳过标点符合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33351075"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_text) # 新的文本字符个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 根据 换行符 \\n 分割\n",
    "clean_text = clean_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签 根据 \\n 分割\n",
    "labels = labels.split('\\n')\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative', 'positive', 'negative', 'positive']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 字典： 单词 --> 索引\n",
    "\n",
    "# 获取所有评论中的每个单词\n",
    "words = [word.lower() for sentence in clean_text for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '', 'it', 'ran', 'at']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10] # 显示前10个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_words = list(set(words)) # 筛选出所有评论中不同的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_words.remove('') # 清理空字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74072"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(various_words) # 不同的单词个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建字典，格式： 单词 ： 整数\n",
    "\n",
    "int_word = dict(enumerate(various_words, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'jgar',\n",
       " 2: 'centrist',\n",
       " 3: 'piteously',\n",
       " 4: 'talmadges',\n",
       " 5: 'confidential',\n",
       " 6: 'flickerino',\n",
       " 7: 'kleine',\n",
       " 8: 'narrations',\n",
       " 9: 'woodlanders',\n",
       " 10: 'personal',\n",
       " 11: 'hardback',\n",
       " 12: 'verbatim',\n",
       " 13: 'disputed',\n",
       " 14: 'indiania',\n",
       " 15: 'ney',\n",
       " 16: 'relatives',\n",
       " 17: 'tintin',\n",
       " 18: 'dollying',\n",
       " 19: 'fuher',\n",
       " 20: 'diplomat',\n",
       " 21: 'cork',\n",
       " 22: 'screamer',\n",
       " 23: 'basterds',\n",
       " 24: 'retrieval',\n",
       " 25: 'colours',\n",
       " 26: 'ssst',\n",
       " 27: 'emphasise',\n",
       " 28: 'humdinger',\n",
       " 29: 'pleaseee',\n",
       " 30: 'adherents',\n",
       " 31: 'worf',\n",
       " 32: 'altro',\n",
       " 33: 'uglying',\n",
       " 34: 'nuovo',\n",
       " 35: 'deepak',\n",
       " 36: 'passions',\n",
       " 37: 'alkie',\n",
       " 38: 'notle',\n",
       " 39: 'revisitation',\n",
       " 40: 'abuses',\n",
       " 41: 'inquisitive',\n",
       " 42: 'guarded',\n",
       " 43: 'pissing',\n",
       " 44: 'tmtm',\n",
       " 45: 'abovementioned',\n",
       " 46: 'feroze',\n",
       " 47: 'lair',\n",
       " 48: 'garrard',\n",
       " 49: 'consort',\n",
       " 50: 'unquiet',\n",
       " 51: 'jiggly',\n",
       " 52: 'lockstock',\n",
       " 53: 'ktla',\n",
       " 54: 'mcconnell',\n",
       " 55: 'yeon',\n",
       " 56: 'maryln',\n",
       " 57: 'emu',\n",
       " 58: 'trim',\n",
       " 59: 'kombat',\n",
       " 60: 'mathematical',\n",
       " 61: 'dogmas',\n",
       " 62: 'glassed',\n",
       " 63: 'microwaving',\n",
       " 64: 'voyeurism',\n",
       " 65: 'failing',\n",
       " 66: 'gildersleeves',\n",
       " 67: 'emery',\n",
       " 68: 'pantheon',\n",
       " 69: 'galleys',\n",
       " 70: 'shiu',\n",
       " 71: 'voiceovers',\n",
       " 72: 'laguna',\n",
       " 73: 'accidence',\n",
       " 74: 'unrequited',\n",
       " 75: 'celebrating',\n",
       " 76: 'kaleidoscope',\n",
       " 77: 'strippers',\n",
       " 78: 'nurse',\n",
       " 79: 'cinemagraphic',\n",
       " 80: 'iamaseal',\n",
       " 81: 'beeblebrox',\n",
       " 82: 'insincerity',\n",
       " 83: 'ooooooh',\n",
       " 84: 'gouald',\n",
       " 85: 'queuing',\n",
       " 86: 'medusin',\n",
       " 87: 'includes',\n",
       " 88: 'corrigan',\n",
       " 89: 'oliveira',\n",
       " 90: 'maharashtrian',\n",
       " 91: 'kirby',\n",
       " 92: 'apc',\n",
       " 93: 'links',\n",
       " 94: 'hackenstien',\n",
       " 95: 'betrayed',\n",
       " 96: 'smirk',\n",
       " 97: 'zealots',\n",
       " 98: 'maple',\n",
       " 99: 'fr',\n",
       " 100: 'fedora',\n",
       " 101: 'dinah',\n",
       " 102: 'vainly',\n",
       " 103: 'vapours',\n",
       " 104: 'satanised',\n",
       " 105: 'overtly',\n",
       " 106: 'telethons',\n",
       " 107: 'secularism',\n",
       " 108: 'departs',\n",
       " 109: 'sunburst',\n",
       " 110: 'recovering',\n",
       " 111: 'rotated',\n",
       " 112: 'pando',\n",
       " 113: 'wonka',\n",
       " 114: 'slake',\n",
       " 115: 'beckoned',\n",
       " 116: 'invalidity',\n",
       " 117: 'lineal',\n",
       " 118: 'singer',\n",
       " 119: 'cavett',\n",
       " 120: 'exsist',\n",
       " 121: 'ratbatspidercrab',\n",
       " 122: 'dominates',\n",
       " 123: 'denigrating',\n",
       " 124: 'loyal',\n",
       " 125: 'grotesque',\n",
       " 126: 'agricultural',\n",
       " 127: 'neelix',\n",
       " 128: 'licitates',\n",
       " 129: 'pirahna',\n",
       " 130: 'installs',\n",
       " 131: 'macaw',\n",
       " 132: 'sorrowful',\n",
       " 133: 'convertible',\n",
       " 134: 'fetid',\n",
       " 135: 'refurbish',\n",
       " 136: 'quicker',\n",
       " 137: 'pledges',\n",
       " 138: 'bocabonita',\n",
       " 139: 'vancouver',\n",
       " 140: 'caddyshack',\n",
       " 141: 'ieuan',\n",
       " 142: 'barters',\n",
       " 143: 'pope',\n",
       " 144: 'ondrej',\n",
       " 145: 'famarialy',\n",
       " 146: 'besxt',\n",
       " 147: 'mako',\n",
       " 148: 'corset',\n",
       " 149: 'leterrier',\n",
       " 150: 'vinod',\n",
       " 151: 'telescopic',\n",
       " 152: 'uncommon',\n",
       " 153: 'harlins',\n",
       " 154: 'utterance',\n",
       " 155: 'woof',\n",
       " 156: 'impolite',\n",
       " 157: 'surprisethrough',\n",
       " 158: 'adherence',\n",
       " 159: 'dumbo',\n",
       " 160: 'krissakes',\n",
       " 161: 'champ',\n",
       " 162: 'legerdemain',\n",
       " 163: 'uneven',\n",
       " 164: 'sexagenarians',\n",
       " 165: 'gaynor',\n",
       " 166: 'rooney',\n",
       " 167: 'cardz',\n",
       " 168: 'looking',\n",
       " 169: 'narrator',\n",
       " 170: 'buffy',\n",
       " 171: 'ashlee',\n",
       " 172: 'poil',\n",
       " 173: 'dissipating',\n",
       " 174: 'autant',\n",
       " 175: 'playground',\n",
       " 176: 'superimposition',\n",
       " 177: 'tucson',\n",
       " 178: 'carface',\n",
       " 179: 'jw',\n",
       " 180: 'untamed',\n",
       " 181: 'fantasticaly',\n",
       " 182: 'precisely',\n",
       " 183: 'ait',\n",
       " 184: 'wowed',\n",
       " 185: 'tc',\n",
       " 186: 'elicot',\n",
       " 187: 'unsure',\n",
       " 188: 'sexual',\n",
       " 189: 'phantom',\n",
       " 190: 'transporter',\n",
       " 191: 'prejudice',\n",
       " 192: 'landau',\n",
       " 193: 'visage',\n",
       " 194: 'mindscrewing',\n",
       " 195: 'numbly',\n",
       " 196: 'darken',\n",
       " 197: 'vulpi',\n",
       " 198: 'deever',\n",
       " 199: 'educators',\n",
       " 200: 'ific',\n",
       " 201: 'windmills',\n",
       " 202: 'artyfartyrati',\n",
       " 203: 'albright',\n",
       " 204: 'roundtree',\n",
       " 205: 'sisterly',\n",
       " 206: 'pubert',\n",
       " 207: 'addy',\n",
       " 208: 'demerit',\n",
       " 209: 'schulberg',\n",
       " 210: 'greedo',\n",
       " 211: 'stalag',\n",
       " 212: 'stupendously',\n",
       " 213: 'driller',\n",
       " 214: 'rashad',\n",
       " 215: 'tester',\n",
       " 216: 'driving',\n",
       " 217: 'mesmerization',\n",
       " 218: 'wearily',\n",
       " 219: 'sequals',\n",
       " 220: 'subjugated',\n",
       " 221: 'infants',\n",
       " 222: 'barbecued',\n",
       " 223: 'tried',\n",
       " 224: 'scribblings',\n",
       " 225: 'karsis',\n",
       " 226: 'sexier',\n",
       " 227: 'doers',\n",
       " 228: 'meta',\n",
       " 229: 'warnerscope',\n",
       " 230: 'namedropping',\n",
       " 231: 'ineptitude',\n",
       " 232: 'intro',\n",
       " 233: 'airspace',\n",
       " 234: 'oaf',\n",
       " 235: 'bronson',\n",
       " 236: 'evita',\n",
       " 237: 'crucified',\n",
       " 238: 'wali',\n",
       " 239: 'ada',\n",
       " 240: 'fizzle',\n",
       " 241: 'sable',\n",
       " 242: 'timsit',\n",
       " 243: 'pavlovian',\n",
       " 244: 'upgraded',\n",
       " 245: 'feedback',\n",
       " 246: 'crepe',\n",
       " 247: 'blithely',\n",
       " 248: 'diabolique',\n",
       " 249: 'word',\n",
       " 250: 'razorfriendly',\n",
       " 251: 'prisoners',\n",
       " 252: 'mireille',\n",
       " 253: 'gosford',\n",
       " 254: 'rampage',\n",
       " 255: 'poseiden',\n",
       " 256: 'imperioli',\n",
       " 257: 'sexless',\n",
       " 258: 'sentai',\n",
       " 259: 'vee',\n",
       " 260: 'exacts',\n",
       " 261: 'isotopes',\n",
       " 262: 'acin',\n",
       " 263: 'hone',\n",
       " 264: 'fatone',\n",
       " 265: 'traped',\n",
       " 266: 'huzzah',\n",
       " 267: 'puny',\n",
       " 268: 'luminosity',\n",
       " 269: 'marmalade',\n",
       " 270: 'sesame',\n",
       " 271: 'bulette',\n",
       " 272: 'bended',\n",
       " 273: 'paging',\n",
       " 274: 'rhodes',\n",
       " 275: 'gluing',\n",
       " 276: 'huggers',\n",
       " 277: 'augustus',\n",
       " 278: 'think',\n",
       " 279: 'disruption',\n",
       " 280: 'affability',\n",
       " 281: 'harrisonfirst',\n",
       " 282: 'treacherous',\n",
       " 283: 'robotboy',\n",
       " 284: 'procreation',\n",
       " 285: 'dester',\n",
       " 286: 'segway',\n",
       " 287: 'boulders',\n",
       " 288: 'stitchin',\n",
       " 289: 'brainiac',\n",
       " 290: 'cid',\n",
       " 291: 'plugs',\n",
       " 292: 'jame',\n",
       " 293: 'things',\n",
       " 294: 'irrational',\n",
       " 295: 'culpability',\n",
       " 296: 'weightlifting',\n",
       " 297: 'hars',\n",
       " 298: 'bookdom',\n",
       " 299: 'depiction',\n",
       " 300: 'before',\n",
       " 301: 'inflation',\n",
       " 302: 'overexciting',\n",
       " 303: 'clockwork',\n",
       " 304: 'snapshot',\n",
       " 305: 'jymn',\n",
       " 306: 'togar',\n",
       " 307: 'medicine',\n",
       " 308: 'eaves',\n",
       " 309: 'redo',\n",
       " 310: 'transitioning',\n",
       " 311: 'excessively',\n",
       " 312: 'moroder',\n",
       " 313: 'turrco',\n",
       " 314: 'component',\n",
       " 315: 'avengers',\n",
       " 316: 'sodeberg',\n",
       " 317: 'stiffler',\n",
       " 318: 'shimizu',\n",
       " 319: 'weepers',\n",
       " 320: 'rivero',\n",
       " 321: 'anette',\n",
       " 322: 'understood',\n",
       " 323: 'jojo',\n",
       " 324: 'nightsky',\n",
       " 325: 'malplace',\n",
       " 326: 'afis',\n",
       " 327: 'dixton',\n",
       " 328: 'gurdebeke',\n",
       " 329: 'bails',\n",
       " 330: 'twisted',\n",
       " 331: 'productions',\n",
       " 332: 'konchalovski',\n",
       " 333: 'jrvet',\n",
       " 334: 'hoky',\n",
       " 335: 'heartrending',\n",
       " 336: 'sevigny',\n",
       " 337: 'tonge',\n",
       " 338: 'video',\n",
       " 339: 'serb',\n",
       " 340: 'csar',\n",
       " 341: 'redesigned',\n",
       " 342: 'perjury',\n",
       " 343: 'bloodedly',\n",
       " 344: 'unamusing',\n",
       " 345: 'prevalent',\n",
       " 346: 'heroics',\n",
       " 347: 'peeped',\n",
       " 348: 'hubba',\n",
       " 349: 'leads',\n",
       " 350: 'harrelson',\n",
       " 351: 'glands',\n",
       " 352: 'resemblance',\n",
       " 353: 'brutalised',\n",
       " 354: 'grandly',\n",
       " 355: 'mediation',\n",
       " 356: 'jacy',\n",
       " 357: 'maniacally',\n",
       " 358: 'lucky',\n",
       " 359: 'sleeve',\n",
       " 360: 'himand',\n",
       " 361: 'motor',\n",
       " 362: 'muppets',\n",
       " 363: 'hatchback',\n",
       " 364: 'misinformation',\n",
       " 365: 'stunner',\n",
       " 366: 'carfare',\n",
       " 367: 'hamada',\n",
       " 368: 'indecent',\n",
       " 369: 'haku',\n",
       " 370: 'rear',\n",
       " 371: 'bushfire',\n",
       " 372: 'nation',\n",
       " 373: 'charley',\n",
       " 374: 'shuts',\n",
       " 375: 'cutbacks',\n",
       " 376: 'hollywoond',\n",
       " 377: 'barty',\n",
       " 378: 'stefanie',\n",
       " 379: 'prostration',\n",
       " 380: 'leering',\n",
       " 381: 'duality',\n",
       " 382: 'kumari',\n",
       " 383: 'denigrates',\n",
       " 384: 'fucus',\n",
       " 385: 'compositional',\n",
       " 386: 'louse',\n",
       " 387: 'schoolkid',\n",
       " 388: 'illuminates',\n",
       " 389: 'ballgame',\n",
       " 390: 'denzel',\n",
       " 391: 'baxtor',\n",
       " 392: 'seus',\n",
       " 393: 'chose',\n",
       " 394: 'coincidentially',\n",
       " 395: 'wheeeew',\n",
       " 396: 'hagan',\n",
       " 397: 'wraparound',\n",
       " 398: 'gradations',\n",
       " 399: 'puncture',\n",
       " 400: 'browsing',\n",
       " 401: 'sharia',\n",
       " 402: 'balloon',\n",
       " 403: 'tangere',\n",
       " 404: 'declaring',\n",
       " 405: 'glo',\n",
       " 406: 'unholy',\n",
       " 407: 'dateless',\n",
       " 408: 'askey',\n",
       " 409: 'maniacal',\n",
       " 410: 'ruining',\n",
       " 411: 'carbonite',\n",
       " 412: 'dalton',\n",
       " 413: 'mckenzie',\n",
       " 414: 'lobotomy',\n",
       " 415: 'snags',\n",
       " 416: 'newgrounds',\n",
       " 417: 'pintilie',\n",
       " 418: 'bartel',\n",
       " 419: 'buice',\n",
       " 420: 'humanises',\n",
       " 421: 'wizened',\n",
       " 422: 'gillham',\n",
       " 423: 'mohabbatein',\n",
       " 424: 'sonzero',\n",
       " 425: 'unadulterated',\n",
       " 426: 'endingis',\n",
       " 427: 'simpleton',\n",
       " 428: 'cheekboned',\n",
       " 429: 'bile',\n",
       " 430: 'westside',\n",
       " 431: 'vdb',\n",
       " 432: 'gabba',\n",
       " 433: 'jarndyce',\n",
       " 434: 'residing',\n",
       " 435: 'freeform',\n",
       " 436: 'movive',\n",
       " 437: 'demo',\n",
       " 438: 'clarifying',\n",
       " 439: 'potentiality',\n",
       " 440: 'cervera',\n",
       " 441: 'rouses',\n",
       " 442: 'julien',\n",
       " 443: 'altaira',\n",
       " 444: 'daimond',\n",
       " 445: 'teamsters',\n",
       " 446: 'tainos',\n",
       " 447: 'enhancer',\n",
       " 448: 'richart',\n",
       " 449: 'seond',\n",
       " 450: 'imani',\n",
       " 451: 'dinning',\n",
       " 452: 'celaschi',\n",
       " 453: 'ripen',\n",
       " 454: 'evilest',\n",
       " 455: 'gerbils',\n",
       " 456: 'vindictively',\n",
       " 457: 'patriotic',\n",
       " 458: 'taos',\n",
       " 459: 'cockazilla',\n",
       " 460: 'crackling',\n",
       " 461: 'unpaid',\n",
       " 462: 'seidl',\n",
       " 463: 'zealousness',\n",
       " 464: 'steeve',\n",
       " 465: 'supernaturals',\n",
       " 466: 'precedes',\n",
       " 467: 'nisha',\n",
       " 468: 'muresans',\n",
       " 469: 'lifetimes',\n",
       " 470: 'filthy',\n",
       " 471: 'toyed',\n",
       " 472: 'blodgett',\n",
       " 473: 'sailes',\n",
       " 474: 'shred',\n",
       " 475: 'deforest',\n",
       " 476: 'sin',\n",
       " 477: 'headtripping',\n",
       " 478: 'sherriff',\n",
       " 479: 'nyfiken',\n",
       " 480: 'incensed',\n",
       " 481: 'tabac',\n",
       " 482: 'crimminy',\n",
       " 483: 'wedgie',\n",
       " 484: 'poops',\n",
       " 485: 'damage',\n",
       " 486: 'yakusyo',\n",
       " 487: 'arnies',\n",
       " 488: 'consenting',\n",
       " 489: 'readjusts',\n",
       " 490: 'sandu',\n",
       " 491: 'coverups',\n",
       " 492: 'mailman',\n",
       " 493: 'bald',\n",
       " 494: 'leaves',\n",
       " 495: 'illiteracy',\n",
       " 496: 'winced',\n",
       " 497: 'territory',\n",
       " 498: 'nicholls',\n",
       " 499: 'pickpocketing',\n",
       " 500: 'jittery',\n",
       " 501: 'wips',\n",
       " 502: 'triomphe',\n",
       " 503: 'heathens',\n",
       " 504: 'nabbed',\n",
       " 505: 'ez',\n",
       " 506: 'temperament',\n",
       " 507: 'farley',\n",
       " 508: 'suckingly',\n",
       " 509: 'ansen',\n",
       " 510: 'nananana',\n",
       " 511: 'sossaman',\n",
       " 512: 'provence',\n",
       " 513: 'bettering',\n",
       " 514: 'metropolises',\n",
       " 515: 'rarest',\n",
       " 516: 'ashanti',\n",
       " 517: 'gulshan',\n",
       " 518: 'quoth',\n",
       " 519: 'dogfight',\n",
       " 520: 'sell',\n",
       " 521: 'xvid',\n",
       " 522: 'flipper',\n",
       " 523: 'truffles',\n",
       " 524: 'ironed',\n",
       " 525: 'emhardt',\n",
       " 526: 'mosaic',\n",
       " 527: 'hasselhoff',\n",
       " 528: 'pigsty',\n",
       " 529: 'bunged',\n",
       " 530: 'thlema',\n",
       " 531: 'immigration',\n",
       " 532: 'fleabag',\n",
       " 533: 'deatn',\n",
       " 534: 'smelt',\n",
       " 535: 'petrified',\n",
       " 536: 'sneering',\n",
       " 537: 'talked',\n",
       " 538: 'doss',\n",
       " 539: 'medalian',\n",
       " 540: 'ask',\n",
       " 541: 'petunias',\n",
       " 542: 'lynda',\n",
       " 543: 'lafanu',\n",
       " 544: 'caging',\n",
       " 545: 'exhaust',\n",
       " 546: 'cinematagraph',\n",
       " 547: 'era',\n",
       " 548: 'shunning',\n",
       " 549: 'vaio',\n",
       " 550: 'loader',\n",
       " 551: 'dosn',\n",
       " 552: 'dango',\n",
       " 553: 'dern',\n",
       " 554: 'folksy',\n",
       " 555: 'ghostbuster',\n",
       " 556: 'beginnings',\n",
       " 557: 'inventions',\n",
       " 558: 'elainor',\n",
       " 559: 'jerks',\n",
       " 560: 'testifying',\n",
       " 561: 'dims',\n",
       " 562: 'mcclinton',\n",
       " 563: 'thinker',\n",
       " 564: 'radtha',\n",
       " 565: 'mistake',\n",
       " 566: 'eszterhas',\n",
       " 567: 'catastrophes',\n",
       " 568: 'ratty',\n",
       " 569: 'yeccch',\n",
       " 570: 'bruise',\n",
       " 571: 'sighted',\n",
       " 572: 'vaulting',\n",
       " 573: 'sogo',\n",
       " 574: 'dialed',\n",
       " 575: 'piedgon',\n",
       " 576: 'unison',\n",
       " 577: 'cubbi',\n",
       " 578: 'christenson',\n",
       " 579: 'deleted',\n",
       " 580: 'schifrin',\n",
       " 581: 'kansas',\n",
       " 582: 'subtractions',\n",
       " 583: 'warehouse',\n",
       " 584: 'calculating',\n",
       " 585: 'leading',\n",
       " 586: 'unratable',\n",
       " 587: 'cronies',\n",
       " 588: 'triviata',\n",
       " 589: 'garfiled',\n",
       " 590: 'burgundians',\n",
       " 591: 'lasciviously',\n",
       " 592: 'energetic',\n",
       " 593: 'kids',\n",
       " 594: 'adriana',\n",
       " 595: 'saldana',\n",
       " 596: 'relax',\n",
       " 597: 'tendons',\n",
       " 598: 'roslyn',\n",
       " 599: 'alterior',\n",
       " 600: 'riba',\n",
       " 601: 'waylon',\n",
       " 602: 'symptom',\n",
       " 603: 'counteracts',\n",
       " 604: 'siphon',\n",
       " 605: 'cgs',\n",
       " 606: 'dissension',\n",
       " 607: 'scarcely',\n",
       " 608: 'ministrations',\n",
       " 609: 'transistions',\n",
       " 610: 'bufoonery',\n",
       " 611: 'krystina',\n",
       " 612: 'predictions',\n",
       " 613: 'liabilities',\n",
       " 614: 'uninhabited',\n",
       " 615: 'ops',\n",
       " 616: 'ritz',\n",
       " 617: 'altruistically',\n",
       " 618: 'intrigueing',\n",
       " 619: 'omissions',\n",
       " 620: 'polarizes',\n",
       " 621: 'tampa',\n",
       " 622: 'agreements',\n",
       " 623: 'koreatown',\n",
       " 624: 'escalating',\n",
       " 625: 'outer',\n",
       " 626: 'understudies',\n",
       " 627: 'pelting',\n",
       " 628: 'euro',\n",
       " 629: 'captain',\n",
       " 630: 'furlong',\n",
       " 631: 'lenka',\n",
       " 632: 'coalwood',\n",
       " 633: 'osterwald',\n",
       " 634: 'zu',\n",
       " 635: 'bestow',\n",
       " 636: 'kellum',\n",
       " 637: 'girard',\n",
       " 638: 'starnberg',\n",
       " 639: 'hoses',\n",
       " 640: 'snubbed',\n",
       " 641: 'soaks',\n",
       " 642: 'consumer',\n",
       " 643: 'nutcases',\n",
       " 644: 'rippling',\n",
       " 645: 'economist',\n",
       " 646: 'chuckling',\n",
       " 647: 'sixth',\n",
       " 648: 'clinkers',\n",
       " 649: 'overbloated',\n",
       " 650: 'xylophone',\n",
       " 651: 'dudettes',\n",
       " 652: 'angers',\n",
       " 653: 'sweeps',\n",
       " 654: 'protesting',\n",
       " 655: 'decried',\n",
       " 656: 'ballz',\n",
       " 657: 'programmation',\n",
       " 658: 'gliss',\n",
       " 659: 'directorial',\n",
       " 660: 'filip',\n",
       " 661: 'lodged',\n",
       " 662: 'missoula',\n",
       " 663: 'compete',\n",
       " 664: 'forrester',\n",
       " 665: 'fragasso',\n",
       " 666: 'daivari',\n",
       " 667: 'dash',\n",
       " 668: 'bruno',\n",
       " 669: 'indefinsibly',\n",
       " 670: 'fools',\n",
       " 671: 'umaga',\n",
       " 672: 'dea',\n",
       " 673: 'wittenborn',\n",
       " 674: 'equipped',\n",
       " 675: 'factories',\n",
       " 676: 'kirilian',\n",
       " 677: 'kompetition',\n",
       " 678: 'estrangement',\n",
       " 679: 'ramme',\n",
       " 680: 'ts',\n",
       " 681: 'paracetamol',\n",
       " 682: 'needed',\n",
       " 683: 'indomitability',\n",
       " 684: 'disclaimer',\n",
       " 685: 'shrew',\n",
       " 686: 'diverge',\n",
       " 687: 'caricaturist',\n",
       " 688: 'charing',\n",
       " 689: 'qwak',\n",
       " 690: 'uncharted',\n",
       " 691: 'calmness',\n",
       " 692: 'subscribe',\n",
       " 693: 'utterly',\n",
       " 694: 'faceful',\n",
       " 695: 'forth',\n",
       " 696: 'aronofsky',\n",
       " 697: 'tan',\n",
       " 698: 'knows',\n",
       " 699: 'moodysson',\n",
       " 700: 'cotta',\n",
       " 701: 'seeing',\n",
       " 702: 'miklos',\n",
       " 703: 'rumore',\n",
       " 704: 'barbershop',\n",
       " 705: 'niceness',\n",
       " 706: 'opt',\n",
       " 707: 'wilting',\n",
       " 708: 'yas',\n",
       " 709: 'kamerdaschaft',\n",
       " 710: 'laird',\n",
       " 711: 'lennart',\n",
       " 712: 'hdv',\n",
       " 713: 'blackpool',\n",
       " 714: 'morgus',\n",
       " 715: 'unheralded',\n",
       " 716: 'grinding',\n",
       " 717: 'timbuktu',\n",
       " 718: 'bullhorns',\n",
       " 719: 'utensils',\n",
       " 720: 'benton',\n",
       " 721: 'relieve',\n",
       " 722: 'firearms',\n",
       " 723: 'gustafson',\n",
       " 724: 'slimy',\n",
       " 725: 'enforce',\n",
       " 726: 'laundrette',\n",
       " 727: 'otaku',\n",
       " 728: 'skipable',\n",
       " 729: 'bottled',\n",
       " 730: 'sloshed',\n",
       " 731: 'vandals',\n",
       " 732: 'stewards',\n",
       " 733: 'beccket',\n",
       " 734: 'acceptance',\n",
       " 735: 'analogous',\n",
       " 736: 'pacifistic',\n",
       " 737: 'nasal',\n",
       " 738: 'campeones',\n",
       " 739: 'dull',\n",
       " 740: 'hayek',\n",
       " 741: 'querelle',\n",
       " 742: 'grizzly',\n",
       " 743: 'patron',\n",
       " 744: 'inconnu',\n",
       " 745: 'cicatillo',\n",
       " 746: 'highland',\n",
       " 747: 'rao',\n",
       " 748: 'gesturing',\n",
       " 749: 'petra',\n",
       " 750: 'skinhead',\n",
       " 751: 'dolphs',\n",
       " 752: 'msr',\n",
       " 753: 'cinmas',\n",
       " 754: 'disclamer',\n",
       " 755: 'sholay',\n",
       " 756: 'impregnation',\n",
       " 757: 'flees',\n",
       " 758: 'groupthink',\n",
       " 759: 'civil',\n",
       " 760: 'jeans',\n",
       " 761: 'vie',\n",
       " 762: 'revolving',\n",
       " 763: 'circuit',\n",
       " 764: 'foment',\n",
       " 765: 'andronicus',\n",
       " 766: 'wroth',\n",
       " 767: 'admirees',\n",
       " 768: 'gunman',\n",
       " 769: 'flashy',\n",
       " 770: 'dominici',\n",
       " 771: 'creditors',\n",
       " 772: 'satanism',\n",
       " 773: 'shakes',\n",
       " 774: 'trojans',\n",
       " 775: 'prosciutto',\n",
       " 776: 'raza',\n",
       " 777: 'levieva',\n",
       " 778: 'burglaries',\n",
       " 779: 'saurian',\n",
       " 780: 'seasoned',\n",
       " 781: 'beren',\n",
       " 782: 'meantime',\n",
       " 783: 'amerindian',\n",
       " 784: 'alp',\n",
       " 785: 'schweiger',\n",
       " 786: 'slacks',\n",
       " 787: 'preservation',\n",
       " 788: 'alejandro',\n",
       " 789: 'galen',\n",
       " 790: 'uckridge',\n",
       " 791: 'bffs',\n",
       " 792: 'stickney',\n",
       " 793: 'smears',\n",
       " 794: 'damaging',\n",
       " 795: 'assured',\n",
       " 796: 'leni',\n",
       " 797: 'centimeters',\n",
       " 798: 'gesellich',\n",
       " 799: 'claudi',\n",
       " 800: 'twangy',\n",
       " 801: 'melody',\n",
       " 802: 'carrere',\n",
       " 803: 'influential',\n",
       " 804: 'founds',\n",
       " 805: 'arty',\n",
       " 806: 'tibetian',\n",
       " 807: 'spoiles',\n",
       " 808: 'nagra',\n",
       " 809: 'stiffjean',\n",
       " 810: 'midnight',\n",
       " 811: 'loudmouthed',\n",
       " 812: 'prior',\n",
       " 813: 'imax',\n",
       " 814: 'thumpers',\n",
       " 815: 'talledega',\n",
       " 816: 'advancements',\n",
       " 817: 'inquire',\n",
       " 818: 'cheeta',\n",
       " 819: 'timecop',\n",
       " 820: 'landfall',\n",
       " 821: 'caricias',\n",
       " 822: 'tears',\n",
       " 823: 'indian',\n",
       " 824: 'worden',\n",
       " 825: 'monkeys',\n",
       " 826: 'reflex',\n",
       " 827: 'postures',\n",
       " 828: 'meltingly',\n",
       " 829: 'shore',\n",
       " 830: 'coarseness',\n",
       " 831: 'cronnie',\n",
       " 832: 'giannini',\n",
       " 833: 'kangho',\n",
       " 834: 'kiriyama',\n",
       " 835: 'bakke',\n",
       " 836: 'drowsiness',\n",
       " 837: 'caulder',\n",
       " 838: 'bandai',\n",
       " 839: 'kharis',\n",
       " 840: 'musicians',\n",
       " 841: 'forearm',\n",
       " 842: 'stills',\n",
       " 843: 'mainstay',\n",
       " 844: 'oscillating',\n",
       " 845: 'clang',\n",
       " 846: 'tatta',\n",
       " 847: 'journalists',\n",
       " 848: 'stanislofsky',\n",
       " 849: 'keymaster',\n",
       " 850: 'dintre',\n",
       " 851: 'cadaverous',\n",
       " 852: 'sears',\n",
       " 853: 'harrer',\n",
       " 854: 'attemps',\n",
       " 855: 'crinkling',\n",
       " 856: 'harriett',\n",
       " 857: 'mewling',\n",
       " 858: 'pugilistic',\n",
       " 859: 'concealed',\n",
       " 860: 'raconteur',\n",
       " 861: 'elton',\n",
       " 862: 'bloomington',\n",
       " 863: 'quease',\n",
       " 864: 'labia',\n",
       " 865: 'pagels',\n",
       " 866: 'reginal',\n",
       " 867: 'violates',\n",
       " 868: 'mlis',\n",
       " 869: 'learns',\n",
       " 870: 'singing',\n",
       " 871: 'monumentous',\n",
       " 872: 'mac',\n",
       " 873: 'assimilates',\n",
       " 874: 'pret',\n",
       " 875: 'rigidity',\n",
       " 876: 'regrouping',\n",
       " 877: 'ss',\n",
       " 878: 'backsliding',\n",
       " 879: 'peacetime',\n",
       " 880: 'adorble',\n",
       " 881: 'whisker',\n",
       " 882: 'krista',\n",
       " 883: 'heezy',\n",
       " 884: 'gossipy',\n",
       " 885: 'upham',\n",
       " 886: 'danyael',\n",
       " 887: 'cantinas',\n",
       " 888: 'ferhan',\n",
       " 889: 'grubiness',\n",
       " 890: 'stepping',\n",
       " 891: 'exhilaration',\n",
       " 892: 'mcquack',\n",
       " 893: 'esperanto',\n",
       " 894: 'repetative',\n",
       " 895: 'prolonging',\n",
       " 896: 'selfish',\n",
       " 897: 'lummox',\n",
       " 898: 'toledo',\n",
       " 899: 'videocassette',\n",
       " 900: 'tanny',\n",
       " 901: 'keir',\n",
       " 902: 'magobei',\n",
       " 903: 'intriguingly',\n",
       " 904: 'placing',\n",
       " 905: 'duchess',\n",
       " 906: 'solve',\n",
       " 907: 'transforming',\n",
       " 908: 'sotto',\n",
       " 909: 'diahnn',\n",
       " 910: 'dinocroc',\n",
       " 911: 'kaif',\n",
       " 912: 'acrimony',\n",
       " 913: 'ulysses',\n",
       " 914: 'wallonia',\n",
       " 915: 'channeled',\n",
       " 916: 'onesidedness',\n",
       " 917: 'mcnasty',\n",
       " 918: 'pseudolesbian',\n",
       " 919: 'apallonia',\n",
       " 920: 'narnia',\n",
       " 921: 'organizations',\n",
       " 922: 'loomed',\n",
       " 923: 'inconceivably',\n",
       " 924: 'jail',\n",
       " 925: 'sloppiness',\n",
       " 926: 'mags',\n",
       " 927: 'mainsprings',\n",
       " 928: 'celluloid',\n",
       " 929: 'manichean',\n",
       " 930: 'nou',\n",
       " 931: 'inhumanities',\n",
       " 932: 'gagged',\n",
       " 933: 'klara',\n",
       " 934: 'falon',\n",
       " 935: 'lotsa',\n",
       " 936: 'betwixt',\n",
       " 937: 'archeology',\n",
       " 938: 'shoplifting',\n",
       " 939: 'assumed',\n",
       " 940: 'warden',\n",
       " 941: 'adulthood',\n",
       " 942: 'dispute',\n",
       " 943: 'jalousie',\n",
       " 944: 'persson',\n",
       " 945: 'taxed',\n",
       " 946: 'hokeyness',\n",
       " 947: 'artset',\n",
       " 948: 'prizes',\n",
       " 949: 'conveying',\n",
       " 950: 'airwaves',\n",
       " 951: 'rogers',\n",
       " 952: 'burstingly',\n",
       " 953: 'martinets',\n",
       " 954: 'barbarism',\n",
       " 955: 'preconceptions',\n",
       " 956: 'lechers',\n",
       " 957: 'fanbases',\n",
       " 958: 'overheating',\n",
       " 959: 'newspeak',\n",
       " 960: 'nes',\n",
       " 961: 'cheered',\n",
       " 962: 'stadiums',\n",
       " 963: 'gigantically',\n",
       " 964: 'publishers',\n",
       " 965: 'aimanov',\n",
       " 966: 'cunningly',\n",
       " 967: 'gtavice',\n",
       " 968: 'detested',\n",
       " 969: 'leiberman',\n",
       " 970: 'tudyk',\n",
       " 971: 'krycek',\n",
       " 972: 'uav',\n",
       " 973: 'freelance',\n",
       " 974: 'laughometer',\n",
       " 975: 'squirrel',\n",
       " 976: 'magistral',\n",
       " 977: 'snares',\n",
       " 978: 'captivates',\n",
       " 979: 'bolognus',\n",
       " 980: 'nomenclature',\n",
       " 981: 'goodness',\n",
       " 982: 'frewer',\n",
       " 983: 'romanticising',\n",
       " 984: 'cordova',\n",
       " 985: 'wald',\n",
       " 986: 'matine',\n",
       " 987: 'accentuated',\n",
       " 988: 'mankiewicz',\n",
       " 989: 'goudry',\n",
       " 990: 'seraphic',\n",
       " 991: 'gyro',\n",
       " 992: 'witty',\n",
       " 993: 'refuel',\n",
       " 994: 'threshold',\n",
       " 995: 'solarbabies',\n",
       " 996: 'innkeeper',\n",
       " 997: 'thousands',\n",
       " 998: 'suffrage',\n",
       " 999: 'cods',\n",
       " 1000: 'advertisement',\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典，格式： 整数 ： 单词\n",
    "word_int = {w:int(i) for i, w in int_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jgar': 1,\n",
       " 'centrist': 2,\n",
       " 'piteously': 3,\n",
       " 'talmadges': 4,\n",
       " 'confidential': 5,\n",
       " 'flickerino': 6,\n",
       " 'kleine': 7,\n",
       " 'narrations': 8,\n",
       " 'woodlanders': 9,\n",
       " 'personal': 10,\n",
       " 'hardback': 11,\n",
       " 'verbatim': 12,\n",
       " 'disputed': 13,\n",
       " 'indiania': 14,\n",
       " 'ney': 15,\n",
       " 'relatives': 16,\n",
       " 'tintin': 17,\n",
       " 'dollying': 18,\n",
       " 'fuher': 19,\n",
       " 'diplomat': 20,\n",
       " 'cork': 21,\n",
       " 'screamer': 22,\n",
       " 'basterds': 23,\n",
       " 'retrieval': 24,\n",
       " 'colours': 25,\n",
       " 'ssst': 26,\n",
       " 'emphasise': 27,\n",
       " 'humdinger': 28,\n",
       " 'pleaseee': 29,\n",
       " 'adherents': 30,\n",
       " 'worf': 31,\n",
       " 'altro': 32,\n",
       " 'uglying': 33,\n",
       " 'nuovo': 34,\n",
       " 'deepak': 35,\n",
       " 'passions': 36,\n",
       " 'alkie': 37,\n",
       " 'notle': 38,\n",
       " 'revisitation': 39,\n",
       " 'abuses': 40,\n",
       " 'inquisitive': 41,\n",
       " 'guarded': 42,\n",
       " 'pissing': 43,\n",
       " 'tmtm': 44,\n",
       " 'abovementioned': 45,\n",
       " 'feroze': 46,\n",
       " 'lair': 47,\n",
       " 'garrard': 48,\n",
       " 'consort': 49,\n",
       " 'unquiet': 50,\n",
       " 'jiggly': 51,\n",
       " 'lockstock': 52,\n",
       " 'ktla': 53,\n",
       " 'mcconnell': 54,\n",
       " 'yeon': 55,\n",
       " 'maryln': 56,\n",
       " 'emu': 57,\n",
       " 'trim': 58,\n",
       " 'kombat': 59,\n",
       " 'mathematical': 60,\n",
       " 'dogmas': 61,\n",
       " 'glassed': 62,\n",
       " 'microwaving': 63,\n",
       " 'voyeurism': 64,\n",
       " 'failing': 65,\n",
       " 'gildersleeves': 66,\n",
       " 'emery': 67,\n",
       " 'pantheon': 68,\n",
       " 'galleys': 69,\n",
       " 'shiu': 70,\n",
       " 'voiceovers': 71,\n",
       " 'laguna': 72,\n",
       " 'accidence': 73,\n",
       " 'unrequited': 74,\n",
       " 'celebrating': 75,\n",
       " 'kaleidoscope': 76,\n",
       " 'strippers': 77,\n",
       " 'nurse': 78,\n",
       " 'cinemagraphic': 79,\n",
       " 'iamaseal': 80,\n",
       " 'beeblebrox': 81,\n",
       " 'insincerity': 82,\n",
       " 'ooooooh': 83,\n",
       " 'gouald': 84,\n",
       " 'queuing': 85,\n",
       " 'medusin': 86,\n",
       " 'includes': 87,\n",
       " 'corrigan': 88,\n",
       " 'oliveira': 89,\n",
       " 'maharashtrian': 90,\n",
       " 'kirby': 91,\n",
       " 'apc': 92,\n",
       " 'links': 93,\n",
       " 'hackenstien': 94,\n",
       " 'betrayed': 95,\n",
       " 'smirk': 96,\n",
       " 'zealots': 97,\n",
       " 'maple': 98,\n",
       " 'fr': 99,\n",
       " 'fedora': 100,\n",
       " 'dinah': 101,\n",
       " 'vainly': 102,\n",
       " 'vapours': 103,\n",
       " 'satanised': 104,\n",
       " 'overtly': 105,\n",
       " 'telethons': 106,\n",
       " 'secularism': 107,\n",
       " 'departs': 108,\n",
       " 'sunburst': 109,\n",
       " 'recovering': 110,\n",
       " 'rotated': 111,\n",
       " 'pando': 112,\n",
       " 'wonka': 113,\n",
       " 'slake': 114,\n",
       " 'beckoned': 115,\n",
       " 'invalidity': 116,\n",
       " 'lineal': 117,\n",
       " 'singer': 118,\n",
       " 'cavett': 119,\n",
       " 'exsist': 120,\n",
       " 'ratbatspidercrab': 121,\n",
       " 'dominates': 122,\n",
       " 'denigrating': 123,\n",
       " 'loyal': 124,\n",
       " 'grotesque': 125,\n",
       " 'agricultural': 126,\n",
       " 'neelix': 127,\n",
       " 'licitates': 128,\n",
       " 'pirahna': 129,\n",
       " 'installs': 130,\n",
       " 'macaw': 131,\n",
       " 'sorrowful': 132,\n",
       " 'convertible': 133,\n",
       " 'fetid': 134,\n",
       " 'refurbish': 135,\n",
       " 'quicker': 136,\n",
       " 'pledges': 137,\n",
       " 'bocabonita': 138,\n",
       " 'vancouver': 139,\n",
       " 'caddyshack': 140,\n",
       " 'ieuan': 141,\n",
       " 'barters': 142,\n",
       " 'pope': 143,\n",
       " 'ondrej': 144,\n",
       " 'famarialy': 145,\n",
       " 'besxt': 146,\n",
       " 'mako': 147,\n",
       " 'corset': 148,\n",
       " 'leterrier': 149,\n",
       " 'vinod': 150,\n",
       " 'telescopic': 151,\n",
       " 'uncommon': 152,\n",
       " 'harlins': 153,\n",
       " 'utterance': 154,\n",
       " 'woof': 155,\n",
       " 'impolite': 156,\n",
       " 'surprisethrough': 157,\n",
       " 'adherence': 158,\n",
       " 'dumbo': 159,\n",
       " 'krissakes': 160,\n",
       " 'champ': 161,\n",
       " 'legerdemain': 162,\n",
       " 'uneven': 163,\n",
       " 'sexagenarians': 164,\n",
       " 'gaynor': 165,\n",
       " 'rooney': 166,\n",
       " 'cardz': 167,\n",
       " 'looking': 168,\n",
       " 'narrator': 169,\n",
       " 'buffy': 170,\n",
       " 'ashlee': 171,\n",
       " 'poil': 172,\n",
       " 'dissipating': 173,\n",
       " 'autant': 174,\n",
       " 'playground': 175,\n",
       " 'superimposition': 176,\n",
       " 'tucson': 177,\n",
       " 'carface': 178,\n",
       " 'jw': 179,\n",
       " 'untamed': 180,\n",
       " 'fantasticaly': 181,\n",
       " 'precisely': 182,\n",
       " 'ait': 183,\n",
       " 'wowed': 184,\n",
       " 'tc': 185,\n",
       " 'elicot': 186,\n",
       " 'unsure': 187,\n",
       " 'sexual': 188,\n",
       " 'phantom': 189,\n",
       " 'transporter': 190,\n",
       " 'prejudice': 191,\n",
       " 'landau': 192,\n",
       " 'visage': 193,\n",
       " 'mindscrewing': 194,\n",
       " 'numbly': 195,\n",
       " 'darken': 196,\n",
       " 'vulpi': 197,\n",
       " 'deever': 198,\n",
       " 'educators': 199,\n",
       " 'ific': 200,\n",
       " 'windmills': 201,\n",
       " 'artyfartyrati': 202,\n",
       " 'albright': 203,\n",
       " 'roundtree': 204,\n",
       " 'sisterly': 205,\n",
       " 'pubert': 206,\n",
       " 'addy': 207,\n",
       " 'demerit': 208,\n",
       " 'schulberg': 209,\n",
       " 'greedo': 210,\n",
       " 'stalag': 211,\n",
       " 'stupendously': 212,\n",
       " 'driller': 213,\n",
       " 'rashad': 214,\n",
       " 'tester': 215,\n",
       " 'driving': 216,\n",
       " 'mesmerization': 217,\n",
       " 'wearily': 218,\n",
       " 'sequals': 219,\n",
       " 'subjugated': 220,\n",
       " 'infants': 221,\n",
       " 'barbecued': 222,\n",
       " 'tried': 223,\n",
       " 'scribblings': 224,\n",
       " 'karsis': 225,\n",
       " 'sexier': 226,\n",
       " 'doers': 227,\n",
       " 'meta': 228,\n",
       " 'warnerscope': 229,\n",
       " 'namedropping': 230,\n",
       " 'ineptitude': 231,\n",
       " 'intro': 232,\n",
       " 'airspace': 233,\n",
       " 'oaf': 234,\n",
       " 'bronson': 235,\n",
       " 'evita': 236,\n",
       " 'crucified': 237,\n",
       " 'wali': 238,\n",
       " 'ada': 239,\n",
       " 'fizzle': 240,\n",
       " 'sable': 241,\n",
       " 'timsit': 242,\n",
       " 'pavlovian': 243,\n",
       " 'upgraded': 244,\n",
       " 'feedback': 245,\n",
       " 'crepe': 246,\n",
       " 'blithely': 247,\n",
       " 'diabolique': 248,\n",
       " 'word': 249,\n",
       " 'razorfriendly': 250,\n",
       " 'prisoners': 251,\n",
       " 'mireille': 252,\n",
       " 'gosford': 253,\n",
       " 'rampage': 254,\n",
       " 'poseiden': 255,\n",
       " 'imperioli': 256,\n",
       " 'sexless': 257,\n",
       " 'sentai': 258,\n",
       " 'vee': 259,\n",
       " 'exacts': 260,\n",
       " 'isotopes': 261,\n",
       " 'acin': 262,\n",
       " 'hone': 263,\n",
       " 'fatone': 264,\n",
       " 'traped': 265,\n",
       " 'huzzah': 266,\n",
       " 'puny': 267,\n",
       " 'luminosity': 268,\n",
       " 'marmalade': 269,\n",
       " 'sesame': 270,\n",
       " 'bulette': 271,\n",
       " 'bended': 272,\n",
       " 'paging': 273,\n",
       " 'rhodes': 274,\n",
       " 'gluing': 275,\n",
       " 'huggers': 276,\n",
       " 'augustus': 277,\n",
       " 'think': 278,\n",
       " 'disruption': 279,\n",
       " 'affability': 280,\n",
       " 'harrisonfirst': 281,\n",
       " 'treacherous': 282,\n",
       " 'robotboy': 283,\n",
       " 'procreation': 284,\n",
       " 'dester': 285,\n",
       " 'segway': 286,\n",
       " 'boulders': 287,\n",
       " 'stitchin': 288,\n",
       " 'brainiac': 289,\n",
       " 'cid': 290,\n",
       " 'plugs': 291,\n",
       " 'jame': 292,\n",
       " 'things': 293,\n",
       " 'irrational': 294,\n",
       " 'culpability': 295,\n",
       " 'weightlifting': 296,\n",
       " 'hars': 297,\n",
       " 'bookdom': 298,\n",
       " 'depiction': 299,\n",
       " 'before': 300,\n",
       " 'inflation': 301,\n",
       " 'overexciting': 302,\n",
       " 'clockwork': 303,\n",
       " 'snapshot': 304,\n",
       " 'jymn': 305,\n",
       " 'togar': 306,\n",
       " 'medicine': 307,\n",
       " 'eaves': 308,\n",
       " 'redo': 309,\n",
       " 'transitioning': 310,\n",
       " 'excessively': 311,\n",
       " 'moroder': 312,\n",
       " 'turrco': 313,\n",
       " 'component': 314,\n",
       " 'avengers': 315,\n",
       " 'sodeberg': 316,\n",
       " 'stiffler': 317,\n",
       " 'shimizu': 318,\n",
       " 'weepers': 319,\n",
       " 'rivero': 320,\n",
       " 'anette': 321,\n",
       " 'understood': 322,\n",
       " 'jojo': 323,\n",
       " 'nightsky': 324,\n",
       " 'malplace': 325,\n",
       " 'afis': 326,\n",
       " 'dixton': 327,\n",
       " 'gurdebeke': 328,\n",
       " 'bails': 329,\n",
       " 'twisted': 330,\n",
       " 'productions': 331,\n",
       " 'konchalovski': 332,\n",
       " 'jrvet': 333,\n",
       " 'hoky': 334,\n",
       " 'heartrending': 335,\n",
       " 'sevigny': 336,\n",
       " 'tonge': 337,\n",
       " 'video': 338,\n",
       " 'serb': 339,\n",
       " 'csar': 340,\n",
       " 'redesigned': 341,\n",
       " 'perjury': 342,\n",
       " 'bloodedly': 343,\n",
       " 'unamusing': 344,\n",
       " 'prevalent': 345,\n",
       " 'heroics': 346,\n",
       " 'peeped': 347,\n",
       " 'hubba': 348,\n",
       " 'leads': 349,\n",
       " 'harrelson': 350,\n",
       " 'glands': 351,\n",
       " 'resemblance': 352,\n",
       " 'brutalised': 353,\n",
       " 'grandly': 354,\n",
       " 'mediation': 355,\n",
       " 'jacy': 356,\n",
       " 'maniacally': 357,\n",
       " 'lucky': 358,\n",
       " 'sleeve': 359,\n",
       " 'himand': 360,\n",
       " 'motor': 361,\n",
       " 'muppets': 362,\n",
       " 'hatchback': 363,\n",
       " 'misinformation': 364,\n",
       " 'stunner': 365,\n",
       " 'carfare': 366,\n",
       " 'hamada': 367,\n",
       " 'indecent': 368,\n",
       " 'haku': 369,\n",
       " 'rear': 370,\n",
       " 'bushfire': 371,\n",
       " 'nation': 372,\n",
       " 'charley': 373,\n",
       " 'shuts': 374,\n",
       " 'cutbacks': 375,\n",
       " 'hollywoond': 376,\n",
       " 'barty': 377,\n",
       " 'stefanie': 378,\n",
       " 'prostration': 379,\n",
       " 'leering': 380,\n",
       " 'duality': 381,\n",
       " 'kumari': 382,\n",
       " 'denigrates': 383,\n",
       " 'fucus': 384,\n",
       " 'compositional': 385,\n",
       " 'louse': 386,\n",
       " 'schoolkid': 387,\n",
       " 'illuminates': 388,\n",
       " 'ballgame': 389,\n",
       " 'denzel': 390,\n",
       " 'baxtor': 391,\n",
       " 'seus': 392,\n",
       " 'chose': 393,\n",
       " 'coincidentially': 394,\n",
       " 'wheeeew': 395,\n",
       " 'hagan': 396,\n",
       " 'wraparound': 397,\n",
       " 'gradations': 398,\n",
       " 'puncture': 399,\n",
       " 'browsing': 400,\n",
       " 'sharia': 401,\n",
       " 'balloon': 402,\n",
       " 'tangere': 403,\n",
       " 'declaring': 404,\n",
       " 'glo': 405,\n",
       " 'unholy': 406,\n",
       " 'dateless': 407,\n",
       " 'askey': 408,\n",
       " 'maniacal': 409,\n",
       " 'ruining': 410,\n",
       " 'carbonite': 411,\n",
       " 'dalton': 412,\n",
       " 'mckenzie': 413,\n",
       " 'lobotomy': 414,\n",
       " 'snags': 415,\n",
       " 'newgrounds': 416,\n",
       " 'pintilie': 417,\n",
       " 'bartel': 418,\n",
       " 'buice': 419,\n",
       " 'humanises': 420,\n",
       " 'wizened': 421,\n",
       " 'gillham': 422,\n",
       " 'mohabbatein': 423,\n",
       " 'sonzero': 424,\n",
       " 'unadulterated': 425,\n",
       " 'endingis': 426,\n",
       " 'simpleton': 427,\n",
       " 'cheekboned': 428,\n",
       " 'bile': 429,\n",
       " 'westside': 430,\n",
       " 'vdb': 431,\n",
       " 'gabba': 432,\n",
       " 'jarndyce': 433,\n",
       " 'residing': 434,\n",
       " 'freeform': 435,\n",
       " 'movive': 436,\n",
       " 'demo': 437,\n",
       " 'clarifying': 438,\n",
       " 'potentiality': 439,\n",
       " 'cervera': 440,\n",
       " 'rouses': 441,\n",
       " 'julien': 442,\n",
       " 'altaira': 443,\n",
       " 'daimond': 444,\n",
       " 'teamsters': 445,\n",
       " 'tainos': 446,\n",
       " 'enhancer': 447,\n",
       " 'richart': 448,\n",
       " 'seond': 449,\n",
       " 'imani': 450,\n",
       " 'dinning': 451,\n",
       " 'celaschi': 452,\n",
       " 'ripen': 453,\n",
       " 'evilest': 454,\n",
       " 'gerbils': 455,\n",
       " 'vindictively': 456,\n",
       " 'patriotic': 457,\n",
       " 'taos': 458,\n",
       " 'cockazilla': 459,\n",
       " 'crackling': 460,\n",
       " 'unpaid': 461,\n",
       " 'seidl': 462,\n",
       " 'zealousness': 463,\n",
       " 'steeve': 464,\n",
       " 'supernaturals': 465,\n",
       " 'precedes': 466,\n",
       " 'nisha': 467,\n",
       " 'muresans': 468,\n",
       " 'lifetimes': 469,\n",
       " 'filthy': 470,\n",
       " 'toyed': 471,\n",
       " 'blodgett': 472,\n",
       " 'sailes': 473,\n",
       " 'shred': 474,\n",
       " 'deforest': 475,\n",
       " 'sin': 476,\n",
       " 'headtripping': 477,\n",
       " 'sherriff': 478,\n",
       " 'nyfiken': 479,\n",
       " 'incensed': 480,\n",
       " 'tabac': 481,\n",
       " 'crimminy': 482,\n",
       " 'wedgie': 483,\n",
       " 'poops': 484,\n",
       " 'damage': 485,\n",
       " 'yakusyo': 486,\n",
       " 'arnies': 487,\n",
       " 'consenting': 488,\n",
       " 'readjusts': 489,\n",
       " 'sandu': 490,\n",
       " 'coverups': 491,\n",
       " 'mailman': 492,\n",
       " 'bald': 493,\n",
       " 'leaves': 494,\n",
       " 'illiteracy': 495,\n",
       " 'winced': 496,\n",
       " 'territory': 497,\n",
       " 'nicholls': 498,\n",
       " 'pickpocketing': 499,\n",
       " 'jittery': 500,\n",
       " 'wips': 501,\n",
       " 'triomphe': 502,\n",
       " 'heathens': 503,\n",
       " 'nabbed': 504,\n",
       " 'ez': 505,\n",
       " 'temperament': 506,\n",
       " 'farley': 507,\n",
       " 'suckingly': 508,\n",
       " 'ansen': 509,\n",
       " 'nananana': 510,\n",
       " 'sossaman': 511,\n",
       " 'provence': 512,\n",
       " 'bettering': 513,\n",
       " 'metropolises': 514,\n",
       " 'rarest': 515,\n",
       " 'ashanti': 516,\n",
       " 'gulshan': 517,\n",
       " 'quoth': 518,\n",
       " 'dogfight': 519,\n",
       " 'sell': 520,\n",
       " 'xvid': 521,\n",
       " 'flipper': 522,\n",
       " 'truffles': 523,\n",
       " 'ironed': 524,\n",
       " 'emhardt': 525,\n",
       " 'mosaic': 526,\n",
       " 'hasselhoff': 527,\n",
       " 'pigsty': 528,\n",
       " 'bunged': 529,\n",
       " 'thlema': 530,\n",
       " 'immigration': 531,\n",
       " 'fleabag': 532,\n",
       " 'deatn': 533,\n",
       " 'smelt': 534,\n",
       " 'petrified': 535,\n",
       " 'sneering': 536,\n",
       " 'talked': 537,\n",
       " 'doss': 538,\n",
       " 'medalian': 539,\n",
       " 'ask': 540,\n",
       " 'petunias': 541,\n",
       " 'lynda': 542,\n",
       " 'lafanu': 543,\n",
       " 'caging': 544,\n",
       " 'exhaust': 545,\n",
       " 'cinematagraph': 546,\n",
       " 'era': 547,\n",
       " 'shunning': 548,\n",
       " 'vaio': 549,\n",
       " 'loader': 550,\n",
       " 'dosn': 551,\n",
       " 'dango': 552,\n",
       " 'dern': 553,\n",
       " 'folksy': 554,\n",
       " 'ghostbuster': 555,\n",
       " 'beginnings': 556,\n",
       " 'inventions': 557,\n",
       " 'elainor': 558,\n",
       " 'jerks': 559,\n",
       " 'testifying': 560,\n",
       " 'dims': 561,\n",
       " 'mcclinton': 562,\n",
       " 'thinker': 563,\n",
       " 'radtha': 564,\n",
       " 'mistake': 565,\n",
       " 'eszterhas': 566,\n",
       " 'catastrophes': 567,\n",
       " 'ratty': 568,\n",
       " 'yeccch': 569,\n",
       " 'bruise': 570,\n",
       " 'sighted': 571,\n",
       " 'vaulting': 572,\n",
       " 'sogo': 573,\n",
       " 'dialed': 574,\n",
       " 'piedgon': 575,\n",
       " 'unison': 576,\n",
       " 'cubbi': 577,\n",
       " 'christenson': 578,\n",
       " 'deleted': 579,\n",
       " 'schifrin': 580,\n",
       " 'kansas': 581,\n",
       " 'subtractions': 582,\n",
       " 'warehouse': 583,\n",
       " 'calculating': 584,\n",
       " 'leading': 585,\n",
       " 'unratable': 586,\n",
       " 'cronies': 587,\n",
       " 'triviata': 588,\n",
       " 'garfiled': 589,\n",
       " 'burgundians': 590,\n",
       " 'lasciviously': 591,\n",
       " 'energetic': 592,\n",
       " 'kids': 593,\n",
       " 'adriana': 594,\n",
       " 'saldana': 595,\n",
       " 'relax': 596,\n",
       " 'tendons': 597,\n",
       " 'roslyn': 598,\n",
       " 'alterior': 599,\n",
       " 'riba': 600,\n",
       " 'waylon': 601,\n",
       " 'symptom': 602,\n",
       " 'counteracts': 603,\n",
       " 'siphon': 604,\n",
       " 'cgs': 605,\n",
       " 'dissension': 606,\n",
       " 'scarcely': 607,\n",
       " 'ministrations': 608,\n",
       " 'transistions': 609,\n",
       " 'bufoonery': 610,\n",
       " 'krystina': 611,\n",
       " 'predictions': 612,\n",
       " 'liabilities': 613,\n",
       " 'uninhabited': 614,\n",
       " 'ops': 615,\n",
       " 'ritz': 616,\n",
       " 'altruistically': 617,\n",
       " 'intrigueing': 618,\n",
       " 'omissions': 619,\n",
       " 'polarizes': 620,\n",
       " 'tampa': 621,\n",
       " 'agreements': 622,\n",
       " 'koreatown': 623,\n",
       " 'escalating': 624,\n",
       " 'outer': 625,\n",
       " 'understudies': 626,\n",
       " 'pelting': 627,\n",
       " 'euro': 628,\n",
       " 'captain': 629,\n",
       " 'furlong': 630,\n",
       " 'lenka': 631,\n",
       " 'coalwood': 632,\n",
       " 'osterwald': 633,\n",
       " 'zu': 634,\n",
       " 'bestow': 635,\n",
       " 'kellum': 636,\n",
       " 'girard': 637,\n",
       " 'starnberg': 638,\n",
       " 'hoses': 639,\n",
       " 'snubbed': 640,\n",
       " 'soaks': 641,\n",
       " 'consumer': 642,\n",
       " 'nutcases': 643,\n",
       " 'rippling': 644,\n",
       " 'economist': 645,\n",
       " 'chuckling': 646,\n",
       " 'sixth': 647,\n",
       " 'clinkers': 648,\n",
       " 'overbloated': 649,\n",
       " 'xylophone': 650,\n",
       " 'dudettes': 651,\n",
       " 'angers': 652,\n",
       " 'sweeps': 653,\n",
       " 'protesting': 654,\n",
       " 'decried': 655,\n",
       " 'ballz': 656,\n",
       " 'programmation': 657,\n",
       " 'gliss': 658,\n",
       " 'directorial': 659,\n",
       " 'filip': 660,\n",
       " 'lodged': 661,\n",
       " 'missoula': 662,\n",
       " 'compete': 663,\n",
       " 'forrester': 664,\n",
       " 'fragasso': 665,\n",
       " 'daivari': 666,\n",
       " 'dash': 667,\n",
       " 'bruno': 668,\n",
       " 'indefinsibly': 669,\n",
       " 'fools': 670,\n",
       " 'umaga': 671,\n",
       " 'dea': 672,\n",
       " 'wittenborn': 673,\n",
       " 'equipped': 674,\n",
       " 'factories': 675,\n",
       " 'kirilian': 676,\n",
       " 'kompetition': 677,\n",
       " 'estrangement': 678,\n",
       " 'ramme': 679,\n",
       " 'ts': 680,\n",
       " 'paracetamol': 681,\n",
       " 'needed': 682,\n",
       " 'indomitability': 683,\n",
       " 'disclaimer': 684,\n",
       " 'shrew': 685,\n",
       " 'diverge': 686,\n",
       " 'caricaturist': 687,\n",
       " 'charing': 688,\n",
       " 'qwak': 689,\n",
       " 'uncharted': 690,\n",
       " 'calmness': 691,\n",
       " 'subscribe': 692,\n",
       " 'utterly': 693,\n",
       " 'faceful': 694,\n",
       " 'forth': 695,\n",
       " 'aronofsky': 696,\n",
       " 'tan': 697,\n",
       " 'knows': 698,\n",
       " 'moodysson': 699,\n",
       " 'cotta': 700,\n",
       " 'seeing': 701,\n",
       " 'miklos': 702,\n",
       " 'rumore': 703,\n",
       " 'barbershop': 704,\n",
       " 'niceness': 705,\n",
       " 'opt': 706,\n",
       " 'wilting': 707,\n",
       " 'yas': 708,\n",
       " 'kamerdaschaft': 709,\n",
       " 'laird': 710,\n",
       " 'lennart': 711,\n",
       " 'hdv': 712,\n",
       " 'blackpool': 713,\n",
       " 'morgus': 714,\n",
       " 'unheralded': 715,\n",
       " 'grinding': 716,\n",
       " 'timbuktu': 717,\n",
       " 'bullhorns': 718,\n",
       " 'utensils': 719,\n",
       " 'benton': 720,\n",
       " 'relieve': 721,\n",
       " 'firearms': 722,\n",
       " 'gustafson': 723,\n",
       " 'slimy': 724,\n",
       " 'enforce': 725,\n",
       " 'laundrette': 726,\n",
       " 'otaku': 727,\n",
       " 'skipable': 728,\n",
       " 'bottled': 729,\n",
       " 'sloshed': 730,\n",
       " 'vandals': 731,\n",
       " 'stewards': 732,\n",
       " 'beccket': 733,\n",
       " 'acceptance': 734,\n",
       " 'analogous': 735,\n",
       " 'pacifistic': 736,\n",
       " 'nasal': 737,\n",
       " 'campeones': 738,\n",
       " 'dull': 739,\n",
       " 'hayek': 740,\n",
       " 'querelle': 741,\n",
       " 'grizzly': 742,\n",
       " 'patron': 743,\n",
       " 'inconnu': 744,\n",
       " 'cicatillo': 745,\n",
       " 'highland': 746,\n",
       " 'rao': 747,\n",
       " 'gesturing': 748,\n",
       " 'petra': 749,\n",
       " 'skinhead': 750,\n",
       " 'dolphs': 751,\n",
       " 'msr': 752,\n",
       " 'cinmas': 753,\n",
       " 'disclamer': 754,\n",
       " 'sholay': 755,\n",
       " 'impregnation': 756,\n",
       " 'flees': 757,\n",
       " 'groupthink': 758,\n",
       " 'civil': 759,\n",
       " 'jeans': 760,\n",
       " 'vie': 761,\n",
       " 'revolving': 762,\n",
       " 'circuit': 763,\n",
       " 'foment': 764,\n",
       " 'andronicus': 765,\n",
       " 'wroth': 766,\n",
       " 'admirees': 767,\n",
       " 'gunman': 768,\n",
       " 'flashy': 769,\n",
       " 'dominici': 770,\n",
       " 'creditors': 771,\n",
       " 'satanism': 772,\n",
       " 'shakes': 773,\n",
       " 'trojans': 774,\n",
       " 'prosciutto': 775,\n",
       " 'raza': 776,\n",
       " 'levieva': 777,\n",
       " 'burglaries': 778,\n",
       " 'saurian': 779,\n",
       " 'seasoned': 780,\n",
       " 'beren': 781,\n",
       " 'meantime': 782,\n",
       " 'amerindian': 783,\n",
       " 'alp': 784,\n",
       " 'schweiger': 785,\n",
       " 'slacks': 786,\n",
       " 'preservation': 787,\n",
       " 'alejandro': 788,\n",
       " 'galen': 789,\n",
       " 'uckridge': 790,\n",
       " 'bffs': 791,\n",
       " 'stickney': 792,\n",
       " 'smears': 793,\n",
       " 'damaging': 794,\n",
       " 'assured': 795,\n",
       " 'leni': 796,\n",
       " 'centimeters': 797,\n",
       " 'gesellich': 798,\n",
       " 'claudi': 799,\n",
       " 'twangy': 800,\n",
       " 'melody': 801,\n",
       " 'carrere': 802,\n",
       " 'influential': 803,\n",
       " 'founds': 804,\n",
       " 'arty': 805,\n",
       " 'tibetian': 806,\n",
       " 'spoiles': 807,\n",
       " 'nagra': 808,\n",
       " 'stiffjean': 809,\n",
       " 'midnight': 810,\n",
       " 'loudmouthed': 811,\n",
       " 'prior': 812,\n",
       " 'imax': 813,\n",
       " 'thumpers': 814,\n",
       " 'talledega': 815,\n",
       " 'advancements': 816,\n",
       " 'inquire': 817,\n",
       " 'cheeta': 818,\n",
       " 'timecop': 819,\n",
       " 'landfall': 820,\n",
       " 'caricias': 821,\n",
       " 'tears': 822,\n",
       " 'indian': 823,\n",
       " 'worden': 824,\n",
       " 'monkeys': 825,\n",
       " 'reflex': 826,\n",
       " 'postures': 827,\n",
       " 'meltingly': 828,\n",
       " 'shore': 829,\n",
       " 'coarseness': 830,\n",
       " 'cronnie': 831,\n",
       " 'giannini': 832,\n",
       " 'kangho': 833,\n",
       " 'kiriyama': 834,\n",
       " 'bakke': 835,\n",
       " 'drowsiness': 836,\n",
       " 'caulder': 837,\n",
       " 'bandai': 838,\n",
       " 'kharis': 839,\n",
       " 'musicians': 840,\n",
       " 'forearm': 841,\n",
       " 'stills': 842,\n",
       " 'mainstay': 843,\n",
       " 'oscillating': 844,\n",
       " 'clang': 845,\n",
       " 'tatta': 846,\n",
       " 'journalists': 847,\n",
       " 'stanislofsky': 848,\n",
       " 'keymaster': 849,\n",
       " 'dintre': 850,\n",
       " 'cadaverous': 851,\n",
       " 'sears': 852,\n",
       " 'harrer': 853,\n",
       " 'attemps': 854,\n",
       " 'crinkling': 855,\n",
       " 'harriett': 856,\n",
       " 'mewling': 857,\n",
       " 'pugilistic': 858,\n",
       " 'concealed': 859,\n",
       " 'raconteur': 860,\n",
       " 'elton': 861,\n",
       " 'bloomington': 862,\n",
       " 'quease': 863,\n",
       " 'labia': 864,\n",
       " 'pagels': 865,\n",
       " 'reginal': 866,\n",
       " 'violates': 867,\n",
       " 'mlis': 868,\n",
       " 'learns': 869,\n",
       " 'singing': 870,\n",
       " 'monumentous': 871,\n",
       " 'mac': 872,\n",
       " 'assimilates': 873,\n",
       " 'pret': 874,\n",
       " 'rigidity': 875,\n",
       " 'regrouping': 876,\n",
       " 'ss': 877,\n",
       " 'backsliding': 878,\n",
       " 'peacetime': 879,\n",
       " 'adorble': 880,\n",
       " 'whisker': 881,\n",
       " 'krista': 882,\n",
       " 'heezy': 883,\n",
       " 'gossipy': 884,\n",
       " 'upham': 885,\n",
       " 'danyael': 886,\n",
       " 'cantinas': 887,\n",
       " 'ferhan': 888,\n",
       " 'grubiness': 889,\n",
       " 'stepping': 890,\n",
       " 'exhilaration': 891,\n",
       " 'mcquack': 892,\n",
       " 'esperanto': 893,\n",
       " 'repetative': 894,\n",
       " 'prolonging': 895,\n",
       " 'selfish': 896,\n",
       " 'lummox': 897,\n",
       " 'toledo': 898,\n",
       " 'videocassette': 899,\n",
       " 'tanny': 900,\n",
       " 'keir': 901,\n",
       " 'magobei': 902,\n",
       " 'intriguingly': 903,\n",
       " 'placing': 904,\n",
       " 'duchess': 905,\n",
       " 'solve': 906,\n",
       " 'transforming': 907,\n",
       " 'sotto': 908,\n",
       " 'diahnn': 909,\n",
       " 'dinocroc': 910,\n",
       " 'kaif': 911,\n",
       " 'acrimony': 912,\n",
       " 'ulysses': 913,\n",
       " 'wallonia': 914,\n",
       " 'channeled': 915,\n",
       " 'onesidedness': 916,\n",
       " 'mcnasty': 917,\n",
       " 'pseudolesbian': 918,\n",
       " 'apallonia': 919,\n",
       " 'narnia': 920,\n",
       " 'organizations': 921,\n",
       " 'loomed': 922,\n",
       " 'inconceivably': 923,\n",
       " 'jail': 924,\n",
       " 'sloppiness': 925,\n",
       " 'mags': 926,\n",
       " 'mainsprings': 927,\n",
       " 'celluloid': 928,\n",
       " 'manichean': 929,\n",
       " 'nou': 930,\n",
       " 'inhumanities': 931,\n",
       " 'gagged': 932,\n",
       " 'klara': 933,\n",
       " 'falon': 934,\n",
       " 'lotsa': 935,\n",
       " 'betwixt': 936,\n",
       " 'archeology': 937,\n",
       " 'shoplifting': 938,\n",
       " 'assumed': 939,\n",
       " 'warden': 940,\n",
       " 'adulthood': 941,\n",
       " 'dispute': 942,\n",
       " 'jalousie': 943,\n",
       " 'persson': 944,\n",
       " 'taxed': 945,\n",
       " 'hokeyness': 946,\n",
       " 'artset': 947,\n",
       " 'prizes': 948,\n",
       " 'conveying': 949,\n",
       " 'airwaves': 950,\n",
       " 'rogers': 951,\n",
       " 'burstingly': 952,\n",
       " 'martinets': 953,\n",
       " 'barbarism': 954,\n",
       " 'preconceptions': 955,\n",
       " 'lechers': 956,\n",
       " 'fanbases': 957,\n",
       " 'overheating': 958,\n",
       " 'newspeak': 959,\n",
       " 'nes': 960,\n",
       " 'cheered': 961,\n",
       " 'stadiums': 962,\n",
       " 'gigantically': 963,\n",
       " 'publishers': 964,\n",
       " 'aimanov': 965,\n",
       " 'cunningly': 966,\n",
       " 'gtavice': 967,\n",
       " 'detested': 968,\n",
       " 'leiberman': 969,\n",
       " 'tudyk': 970,\n",
       " 'krycek': 971,\n",
       " 'uav': 972,\n",
       " 'freelance': 973,\n",
       " 'laughometer': 974,\n",
       " 'squirrel': 975,\n",
       " 'magistral': 976,\n",
       " 'snares': 977,\n",
       " 'captivates': 978,\n",
       " 'bolognus': 979,\n",
       " 'nomenclature': 980,\n",
       " 'goodness': 981,\n",
       " 'frewer': 982,\n",
       " 'romanticising': 983,\n",
       " 'cordova': 984,\n",
       " 'wald': 985,\n",
       " 'matine': 986,\n",
       " 'accentuated': 987,\n",
       " 'mankiewicz': 988,\n",
       " 'goudry': 989,\n",
       " 'seraphic': 990,\n",
       " 'gyro': 991,\n",
       " 'witty': 992,\n",
       " 'refuel': 993,\n",
       " 'threshold': 994,\n",
       " 'solarbabies': 995,\n",
       " 'innkeeper': 996,\n",
       " 'thousands': 997,\n",
       " 'suffrage': 998,\n",
       " 'cods': 999,\n",
       " 'advertisement': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 标签 --> 1， 0 转换\n",
    "# positive : 1,  negative : 0\n",
    "\n",
    "label_int = np.array([1 if x == 'positive' else 0 for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 12500, 0: 12501})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(label_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 清理文本太短以及过长的样本\n",
    "\n",
    "# 统计文本中，每条评论的长度\n",
    "sentence_length = [len(sentence.split()) for sentence in clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(sentence_length) # 统计不同长度的评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小评论长度\n",
    "min_sen = min(sorted(counts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大评论长度\n",
    "max_sen = max(sorted(counts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 min 和 max 对应的索引\n",
    "\n",
    "min_index = [i for i, length in enumerate(sentence_length) if length == min_sen[0]]\n",
    "\n",
    "max_index = [i for i, length in enumerate(sentence_length) if length == max_sen[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25000]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3908]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本数量：  25001\n",
      "新文本数量:  25000\n"
     ]
    }
   ],
   "source": [
    "# 根据索引删除文本中过短或过长的评论\n",
    "\n",
    "new_text = np.delete(clean_text, min_index)\n",
    "\n",
    "print(\"原始文本数量： \", len(clean_text))\n",
    "print(\"新文本数量: \", len(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本数量：  25000\n",
      "新文本数量:  24999\n"
     ]
    }
   ],
   "source": [
    "new_text2 = np.delete(new_text, max_index)\n",
    "\n",
    "print(\"原始文本数量： \", len(new_text))\n",
    "print(\"新文本数量: \", len(new_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始标签数量：  25001\n",
      "新标签数量：  24999\n"
     ]
    }
   ],
   "source": [
    "# 同样需要在标签集中根据索引删除对应的标签\n",
    "\n",
    "new_labels = np.delete(label_int, min_index)\n",
    "\n",
    "new_labels = np.delete(new_labels, max_index)\n",
    "\n",
    "print(\"原始标签数量： \", len(label_int))\n",
    "print(\"新标签数量： \", len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 将单词映射为整型\n",
    "\n",
    "text_ints = []\n",
    "for sentence in new_text2:\n",
    "    sample = list()\n",
    "    for word in sentence.split():\n",
    "        int_value = word_int[word] # 获取到单词对应的键\n",
    "        sample.append(int_value)\n",
    "    text_ints.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7267,\n",
       " 40335,\n",
       " 68363,\n",
       " 51508,\n",
       " 39596,\n",
       " 39305,\n",
       " 72091,\n",
       " 68728,\n",
       " 5556,\n",
       " 4157,\n",
       " 58431,\n",
       " 40054,\n",
       " 54028,\n",
       " 12419,\n",
       " 53777,\n",
       " 1651,\n",
       " 15038,\n",
       " 21831,\n",
       " 33552,\n",
       " 47447,\n",
       " 54028,\n",
       " 51827,\n",
       " 62758,\n",
       " 65921,\n",
       " 55270,\n",
       " 4157,\n",
       " 17051,\n",
       " 32040,\n",
       " 56213,\n",
       " 20722,\n",
       " 14740,\n",
       " 65289,\n",
       " 61837,\n",
       " 7267,\n",
       " 40335,\n",
       " 64500,\n",
       " 70199,\n",
       " 68363,\n",
       " 65395,\n",
       " 47644,\n",
       " 14740,\n",
       " 40972,\n",
       " 33569,\n",
       " 68363,\n",
       " 51827,\n",
       " 4157,\n",
       " 55629,\n",
       " 14740,\n",
       " 60871,\n",
       " 23387,\n",
       " 4157,\n",
       " 73423,\n",
       " 24863,\n",
       " 60705,\n",
       " 50611,\n",
       " 3387,\n",
       " 35849,\n",
       " 47980,\n",
       " 58377,\n",
       " 7561,\n",
       " 51827,\n",
       " 64431,\n",
       " 4157,\n",
       " 10577,\n",
       " 65668,\n",
       " 4157,\n",
       " 8857,\n",
       " 66530,\n",
       " 8278,\n",
       " 53415,\n",
       " 20722,\n",
       " 65668,\n",
       " 4157,\n",
       " 73590,\n",
       " 18926,\n",
       " 8943,\n",
       " 11623,\n",
       " 58377,\n",
       " 24863,\n",
       " 2704,\n",
       " 18926,\n",
       " 49266,\n",
       " 4157,\n",
       " 6587,\n",
       " 55270,\n",
       " 69478,\n",
       " 51508,\n",
       " 22784,\n",
       " 38252,\n",
       " 223,\n",
       " 14740,\n",
       " 71943,\n",
       " 11433,\n",
       " 4157,\n",
       " 21831,\n",
       " 18926,\n",
       " 4878,\n",
       " 20563,\n",
       " 5556,\n",
       " 40335,\n",
       " 51508,\n",
       " 11612,\n",
       " 67798,\n",
       " 10312,\n",
       " 18926,\n",
       " 44888,\n",
       " 60468,\n",
       " 14740,\n",
       " 24671,\n",
       " 7909,\n",
       " 65668,\n",
       " 32948,\n",
       " 51827,\n",
       " 22784,\n",
       " 58725,\n",
       " 14740,\n",
       " 7267,\n",
       " 40335,\n",
       " 18926,\n",
       " 63100,\n",
       " 61837,\n",
       " 58629,\n",
       " 34074,\n",
       " 65668,\n",
       " 62758,\n",
       " 58401,\n",
       " 278,\n",
       " 61837,\n",
       " 7267,\n",
       " 40335,\n",
       " 68363,\n",
       " 45846,\n",
       " 25293,\n",
       " 14047,\n",
       " 51508,\n",
       " 46595,\n",
       " 61837,\n",
       " 72091,\n",
       " 73647,\n",
       " 71398]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ints[0] # 第一条评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24999"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_ints) # 总的评论数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 设定统一的文本长度，对整个文本数据中的每条评论进行填充或截断\n",
    "# 设定每条评论固定长度为200个单词，不足的评论用0填充，超过的直接截断\n",
    "\n",
    "def reset_text(text, seq_len):\n",
    "    dataset = np.zeros((len(text), seq_len))\n",
    "    for index, sentence in enumerate(text):\n",
    "        if len(sentence) < seq_len:\n",
    "            dataset[index, :len(sentence)] = sentence\n",
    "        else:\n",
    "            dataset[index, :] = sentence[:seq_len] # 截断\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = reset_text(text_ints, seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24999, 200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7267., 40335., 68363., 51508., 39596., 39305., 72091., 68728.,\n",
       "        5556.,  4157., 58431., 40054., 54028., 12419., 53777.,  1651.,\n",
       "       15038., 21831., 33552., 47447., 54028., 51827., 62758., 65921.,\n",
       "       55270.,  4157., 17051., 32040., 56213., 20722., 14740., 65289.,\n",
       "       61837.,  7267., 40335., 64500., 70199., 68363., 65395., 47644.,\n",
       "       14740., 40972., 33569., 68363., 51827.,  4157., 55629., 14740.,\n",
       "       60871., 23387.,  4157., 73423., 24863., 60705., 50611.,  3387.,\n",
       "       35849., 47980., 58377.,  7561., 51827., 64431.,  4157., 10577.,\n",
       "       65668.,  4157.,  8857., 66530.,  8278., 53415., 20722., 65668.,\n",
       "        4157., 73590., 18926.,  8943., 11623., 58377., 24863.,  2704.,\n",
       "       18926., 49266.,  4157.,  6587., 55270., 69478., 51508., 22784.,\n",
       "       38252.,   223., 14740., 71943., 11433.,  4157., 21831., 18926.,\n",
       "        4878., 20563.,  5556., 40335., 51508., 11612., 67798., 10312.,\n",
       "       18926., 44888., 60468., 14740., 24671.,  7909., 65668., 32948.,\n",
       "       51827., 22784., 58725., 14740.,  7267., 40335., 18926., 63100.,\n",
       "       61837., 58629., 34074., 65668., 62758., 58401.,   278., 61837.,\n",
       "        7267., 40335., 68363., 45846., 25293., 14047., 51508., 46595.,\n",
       "       61837., 72091., 73647., 71398.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 3.1 数据类型转换\n",
    "dataset_tensor = torch.from_numpy(dataset)\n",
    "label_tensor = torch.from_numpy(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24999, 200])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24999])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数： 24999\n",
      "训练样本数： 19999\n",
      "验证样本数： 2500\n",
      "测试样本数： 2500\n"
     ]
    }
   ],
   "source": [
    "# 3.2 数据分割，train, val, test\n",
    "\n",
    "# 总样本数\n",
    "all_samples = len(dataset_tensor)\n",
    "print(\"总样本数：\",all_samples)\n",
    "\n",
    "# 设置比例\n",
    "ratio = 0.8\n",
    "train_size = int(all_samples * 0.8) # 训练样本数\n",
    "print(\"训练样本数：\",train_size)\n",
    "\n",
    "rest_size = all_samples - train_size # 剩余样本数\n",
    "\n",
    "val_size = int(rest_size * 0.5) # 验证样本数\n",
    "print(\"验证样本数：\", val_size)\n",
    "\n",
    "test_size = int(rest_size * 0.5) # 测试样本数\n",
    "print(\"测试样本数：\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取train, val, test 样本\n",
    "\n",
    "# train\n",
    "train = dataset_tensor[:train_size]\n",
    "train_labels = label_tensor[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19999, 200])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19999])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剩余样本\n",
    "rest_samples = dataset_tensor[train_size:]\n",
    "rest_labels = label_tensor[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val = rest_samples[:val_size]\n",
    "val_labels = rest_labels[:val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500, 200])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test = rest_samples[val_size:]\n",
    "test_labels = rest_labels[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500, 200])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 通过DataLoader按批处理数据\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 对数据进行封装：(评论，标签)\n",
    "train_dataset = TensorDataset(train, train_labels)\n",
    "val_dataset = TensorDataset(val, val_labels)\n",
    "test_dataset = TensorDataset(test, test_labels)\n",
    "\n",
    "batch_size = 128\n",
    "# 批处理\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取train中的一批数据\n",
    "data, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 200])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义网络模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers, dropout=0.5):\n",
    "        super(sentiment, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim) # 词嵌入层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        '''\n",
    "        x shape : (batch_size, seq_len, features)\n",
    "        \n",
    "        '''\n",
    "        batch_size = x.size(0) # 获取batch_size\n",
    "        x = x.long() # 类型转换\n",
    "        #print('x shape : ', x.shape) # torch.Size([128, 200])\n",
    "        embeds = self.embedding(x) # 词嵌入表示 \n",
    "        #print('embeds shape : ', embeds.shape) # torch.Size([128, 200, 300])\n",
    "        out, hidden = self.lstm(embeds, hidden) # lstm out shape : (batch_size, seq_len, hidden_dim)\n",
    "        #print('out_1 shape : ', out.shape) # torch.Size([128, 200, 256])\n",
    "        #print('hidden_0 shape : ', hidden[0].shape) # torch.Size([2, 128, 256])\n",
    "        #print('hidden_1 shape : ', hidden[1].shape) # torch.Size([2, 128, 256])\n",
    "        out = out.reshape(-1, self.hidden_dim) # （batch_size * seq_len, hidden_dim）\n",
    "        #print('out_2 shape : ', out.shape) # torch.Size([25600, 256])\n",
    "        out = self.linear(out) # 全连接层 \n",
    "        #print('out_3 shape : ', out.shape) # torch.Size([25600, 1])\n",
    "        sigmoid_out = self.sigmoid(out) #\n",
    "        #print('sigmoid_out_1 shape : ', sigmoid_out.shape) # torch.Size([25600, 1])\n",
    "        sigmoid_out = sigmoid_out.reshape(batch_size, -1)\n",
    "        #print('sigmoid_out_2 shape : ', sigmoid_out.shape) # torch.Size([128, 200])\n",
    "        sigmoid_out = sigmoid_out[:, -1] # 获取最后一批的标签\n",
    "        #print('sigmoid_out_3 shape : ', sigmoid_out.shape) # torch.Size([128])\n",
    "        return sigmoid_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #print(\"weghit :\", weight.shape) # torch.Size([74073, 300])\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化超参数\n",
    "input_size = len(word_int) + 1 # 输入（不同的单词个数）\n",
    "output_size = 1 # 输出\n",
    "embedding_dim = 400 # 词嵌入维度\n",
    "hidden_dim = 128 # 隐藏层节点个数\n",
    "num_layers = 2 # lstm的层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = sentiment(input_size, embedding_dim, hidden_dim, output_size, num_layers)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss() # 损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 优化器\n",
    "num_epochs = 50 # 循环次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练模型\n",
    "def train(model, device, data_loader, criterion, optimizer, num_epochs, val_loader):\n",
    "    history = list()\n",
    "    for epoch in range(num_epochs):\n",
    "        hs = model.init_hidden(batch_size)\n",
    "        train_loss = []\n",
    "        train_correct = 0.0\n",
    "        model.train()\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device) # 部署到device\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad() # 梯度置零\n",
    "            output, hs = model(data, hs) # 模型训练\n",
    "            hs = tuple([h.data for h in hs])\n",
    "            #print('output shape : ', output.shape) # torch.Size([128])\n",
    "            loss = criterion(output, target.float()) # 计算损失\n",
    "            train_loss.append(loss.item()) # 累计损失\n",
    "            loss.backward() # 反向传播\n",
    "            optimizer.step() # 参数更新\n",
    "            train_correct += torch.sum(output==target) # 比较\n",
    "            \n",
    "        # 模型验证\n",
    "        model.eval()\n",
    "        hs = model.init_hidden(batch_size)\n",
    "        val_loss = []\n",
    "        val_correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                preds, hs = model(data, hs) # 验证\n",
    "                hs = tuple([h.data for h in hs])\n",
    "                loss = criterion(preds, target.float()) # 计算损失\n",
    "                val_loss.append(loss.item()) # 累计损失\n",
    "                val_correct += torch.sum(preds==target) # 比较\n",
    "#             history['val_loss'].append(np.mean(val_loss))\n",
    "#             history['val_correct'].append(np.mean(val_correct))\n",
    "#         history['train_loss'].append(np.mean(train_loss))\n",
    "#         history['train_correct'].append(np.mean(train_correct))\n",
    "        print(f'Epoch {epoch}/{num_epochs} --- train loss {np.round(np.mean(train_loss), 5)} --- val loss {np.round(np.mean(val_loss),5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50 --- train loss 0.69579 --- val loss 0.69153\n",
      "Epoch 1/50 --- train loss 0.65006 --- val loss 0.61376\n",
      "Epoch 2/50 --- train loss 0.50846 --- val loss 0.63338\n",
      "Epoch 3/50 --- train loss 0.41366 --- val loss 0.573\n",
      "Epoch 4/50 --- train loss 0.31818 --- val loss 0.55107\n",
      "Epoch 5/50 --- train loss 0.23296 --- val loss 0.58043\n",
      "Epoch 6/50 --- train loss 0.19104 --- val loss 0.73331\n",
      "Epoch 7/50 --- train loss 0.16312 --- val loss 0.69132\n",
      "Epoch 8/50 --- train loss 0.1371 --- val loss 0.81286\n",
      "Epoch 9/50 --- train loss 0.12594 --- val loss 0.80081\n",
      "Epoch 10/50 --- train loss 0.11844 --- val loss 0.89682\n",
      "Epoch 11/50 --- train loss 0.12253 --- val loss 0.82829\n",
      "Epoch 12/50 --- train loss 0.12665 --- val loss 0.81739\n",
      "Epoch 13/50 --- train loss 0.10503 --- val loss 1.06689\n",
      "Epoch 14/50 --- train loss 0.11575 --- val loss 1.04593\n",
      "Epoch 15/50 --- train loss 0.09994 --- val loss 0.77949\n",
      "Epoch 16/50 --- train loss 0.10297 --- val loss 1.07149\n",
      "Epoch 17/50 --- train loss 0.09307 --- val loss 1.03334\n",
      "Epoch 18/50 --- train loss 0.08066 --- val loss 1.1166\n",
      "Epoch 19/50 --- train loss 0.0733 --- val loss 1.0762\n",
      "Epoch 20/50 --- train loss 0.06231 --- val loss 1.3163\n",
      "Epoch 21/50 --- train loss 0.09275 --- val loss 1.04837\n",
      "Epoch 22/50 --- train loss 0.06788 --- val loss 1.22008\n",
      "Epoch 23/50 --- train loss 0.06058 --- val loss 1.14502\n",
      "Epoch 24/50 --- train loss 0.0503 --- val loss 1.10406\n",
      "Epoch 25/50 --- train loss 0.04811 --- val loss 1.27056\n",
      "Epoch 26/50 --- train loss 0.04678 --- val loss 1.31766\n",
      "Epoch 27/50 --- train loss 0.0417 --- val loss 1.24131\n",
      "Epoch 28/50 --- train loss 0.06402 --- val loss 1.12226\n",
      "Epoch 29/50 --- train loss 0.05651 --- val loss 1.13431\n",
      "Epoch 30/50 --- train loss 0.05637 --- val loss 1.18953\n",
      "Epoch 31/50 --- train loss 0.07677 --- val loss 1.28528\n",
      "Epoch 32/50 --- train loss 0.07089 --- val loss 1.36054\n",
      "Epoch 33/50 --- train loss 0.0521 --- val loss 1.26252\n",
      "Epoch 34/50 --- train loss 0.05338 --- val loss 1.2563\n",
      "Epoch 35/50 --- train loss 0.04381 --- val loss 1.3222\n",
      "Epoch 36/50 --- train loss 0.03912 --- val loss 1.38663\n",
      "Epoch 37/50 --- train loss 0.03755 --- val loss 1.42482\n",
      "Epoch 38/50 --- train loss 0.03783 --- val loss 1.48139\n",
      "Epoch 39/50 --- train loss 0.04054 --- val loss 1.35192\n",
      "Epoch 40/50 --- train loss 0.03643 --- val loss 1.4666\n",
      "Epoch 41/50 --- train loss 0.03497 --- val loss 1.43595\n",
      "Epoch 42/50 --- train loss 0.04161 --- val loss 1.75113\n",
      "Epoch 43/50 --- train loss 0.04434 --- val loss 1.42092\n",
      "Epoch 44/50 --- train loss 0.05225 --- val loss 1.4397\n",
      "Epoch 45/50 --- train loss 0.04518 --- val loss 1.38717\n",
      "Epoch 46/50 --- train loss 0.0427 --- val loss 1.41636\n",
      "Epoch 47/50 --- train loss 0.0463 --- val loss 1.37832\n",
      "Epoch 48/50 --- train loss 0.03953 --- val loss 1.41839\n",
      "Epoch 49/50 --- train loss 0.0347 --- val loss 1.53719\n"
     ]
    }
   ],
   "source": [
    "train(model, device, train_loader, criterion, optimizer, num_epochs, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "\n",
    "def test(model, data_loader, device, criterion):\n",
    "    test_losses = []\n",
    "    num_correct = 0\n",
    "    # 初始化隐藏状态\n",
    "    hs = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    for i, dataset in enumerate(data_loader):\n",
    "        data = dataset[0].to(device) # 部署到device\n",
    "        target = dataset[1].to(device)\n",
    "        output, hs = model(data, hs) # 测试\n",
    "        loss = criterion(output, target.float()) # 计算损失\n",
    "        pred = torch.round(output) # 将预测值进行四舍五入，转换为0 或 1\n",
    "        test_losses.append(loss.item()) # 保存损失\n",
    "        correct_tensor = pred.eq(target.float().view_as(pred)) # 返回一堆True 或 False\n",
    "        correct = correct_tensor.cpu().numpy()\n",
    "        result = np.sum(correct)\n",
    "        num_correct += result\n",
    "        #print(\"num correct : \", num_correct)\n",
    "        print(f'Batch {i}')\n",
    "        print(f'loss : {np.round(np.mean(loss.item()), 3)}')\n",
    "        print(f'accuracy : {np.round(result / len(data), 3) * 100} %')\n",
    "        print()\n",
    "    print(\"总的测试损失 test loss : {:.2f}\".format(np.mean(test_losses)))\n",
    "    print(\"总的测试准确率 test accuracy : {:.2f}\".format(np.mean(num_correct / len(data_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "loss : 1.749\n",
      "accuracy : 68.0 %\n",
      "\n",
      "Batch 1\n",
      "loss : 1.57\n",
      "accuracy : 71.1 %\n",
      "\n",
      "Batch 2\n",
      "loss : 1.489\n",
      "accuracy : 72.7 %\n",
      "\n",
      "Batch 3\n",
      "loss : 1.456\n",
      "accuracy : 75.0 %\n",
      "\n",
      "Batch 4\n",
      "loss : 1.201\n",
      "accuracy : 77.3 %\n",
      "\n",
      "Batch 5\n",
      "loss : 1.432\n",
      "accuracy : 73.4 %\n",
      "\n",
      "Batch 6\n",
      "loss : 1.409\n",
      "accuracy : 74.2 %\n",
      "\n",
      "Batch 7\n",
      "loss : 1.507\n",
      "accuracy : 71.1 %\n",
      "\n",
      "Batch 8\n",
      "loss : 1.626\n",
      "accuracy : 70.3 %\n",
      "\n",
      "Batch 9\n",
      "loss : 1.331\n",
      "accuracy : 77.3 %\n",
      "\n",
      "Batch 10\n",
      "loss : 1.654\n",
      "accuracy : 71.89999999999999 %\n",
      "\n",
      "Batch 11\n",
      "loss : 1.355\n",
      "accuracy : 75.0 %\n",
      "\n",
      "Batch 12\n",
      "loss : 1.45\n",
      "accuracy : 72.7 %\n",
      "\n",
      "Batch 13\n",
      "loss : 1.737\n",
      "accuracy : 67.2 %\n",
      "\n",
      "Batch 14\n",
      "loss : 1.577\n",
      "accuracy : 72.7 %\n",
      "\n",
      "Batch 15\n",
      "loss : 2.027\n",
      "accuracy : 65.60000000000001 %\n",
      "\n",
      "Batch 16\n",
      "loss : 1.747\n",
      "accuracy : 67.2 %\n",
      "\n",
      "Batch 17\n",
      "loss : 1.482\n",
      "accuracy : 75.8 %\n",
      "\n",
      "Batch 18\n",
      "loss : 1.498\n",
      "accuracy : 69.5 %\n",
      "\n",
      "总的测试损失 test loss : 1.54\n",
      "总的测试准确率 test accuracy : 0.70\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader, device, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测（测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例1\n",
    "text = 'this movie is so amazing. the plot is attractive. and I really like it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步：文本转索引（整数）\n",
    "from string import punctuation\n",
    "\n",
    "def converts(text):\n",
    "    # 去除标点符号\n",
    "    new_text = ''.join([char for char in text if char not in punctuation])\n",
    "    print(\"new text :\\n\", new_text)\n",
    "    # 文本映射为索引\n",
    "    text_ints = [word_int[word.lower()] for word in new_text.split()]\n",
    "    print(\"文本映射为索引：\\n\", text_ints)\n",
    "    return text_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new text :\n",
      " this movie is so amazing the plot is attractive and I really like it\n",
      "文本映射为索引：\n",
      " [12542, 26331, 68363, 55476, 13980, 4157, 3491, 68363, 25499, 11623, 18926, 8585, 69980, 72091]\n"
     ]
    }
   ],
   "source": [
    "text_ints = converts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12542,\n",
       " 26331,\n",
       " 68363,\n",
       " 55476,\n",
       " 13980,\n",
       " 4157,\n",
       " 3491,\n",
       " 68363,\n",
       " 25499,\n",
       " 11623,\n",
       " 18926,\n",
       " 8585,\n",
       " 69980,\n",
       " 72091]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本对齐，sequence_length = 200\n",
    "new_text_ints = reset_text([text_ints], seq_len=200) # 注意这里要添加一个[]，因为，reset_text处理的二维数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12542., 26331., 68363., 55476., 13980.,  4157.,  3491., 68363.,\n",
       "        25499., 11623., 18926.,  8585., 69980., 72091.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_ints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# numpy --> tensor\n",
    "text_tensor = torch.from_numpy(new_text_ints)\n",
    "\n",
    "print(text_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预测函数\n",
    "def predict(model, text_tensor, device):\n",
    "    batch_size = text_tensor.size(0) # 这里是1\n",
    "    hs = model.init_hidden(batch_size) # 初始化隐藏状态\n",
    "    text_tensor = text_tensor.to(device)\n",
    "    pred, hs = model(text_tensor, hs) # 判断\n",
    "    print(\"概率值：\", pred.item())\n",
    "    # 将pred概率值转换为0或1\n",
    "    pred = torch.round(pred)\n",
    "    print(\"类别值：\", pred.item())\n",
    "    # 判断\n",
    "    if pred.data == 1:\n",
    "        print(\"评论正面\")\n",
    "    else:\n",
    "        print(\"评论反面\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "概率值： 0.839598536491394\n",
      "类别值： 1.0\n",
      "评论正面\n"
     ]
    }
   ],
   "source": [
    "predict(model, text_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
